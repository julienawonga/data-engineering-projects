# ğŸ“š **README.md â€” Projet RetailFlow : Stack Data Engineering**

## ğŸš€ **1. Introduction**

**RetailFlow** est un projet de **Data Engineering complet** conÃ§u pour simuler le traitement, l'intÃ©gration, la transformation, et l'analyse de donnÃ©es dans le secteur du **retail (commerce de dÃ©tail)**. Ce projet illustre comment collecter, centraliser et analyser des donnÃ©es transactionnelles et des fichiers externes (CSV) Ã  l'aide d'une stack moderne de **Data Engineering open-source**, dÃ©ployÃ©e localement avec **Docker**.

### ğŸ¯ **Objectif du Projet :**
1. **IntÃ©grer des donnÃ©es transactionnelles MySQL** (donnÃ©es clients, produits, commandes).  
2. **Importer des donnÃ©es externes via un fichier CSV** (localisation des magasins).  
3. **Stocker les donnÃ©es dans un Data Warehouse (PostgreSQL)**.  
4. **Orchestrer les flux de donnÃ©es avec Airflow**.  
5. **Transformer et modÃ©liser les donnÃ©es avec dbt**.  
6. **Visualiser les insights avec Apache Superset**.  
7. **Documenter et gouverner les donnÃ©es avec OpenMetadata**.

---

## ğŸ¢ **2. Contexte MÃ©tier : Cas d'Usage Retail**

![Architecture Diagramme](./static/projet-retailflow.png)

### ğŸ“Š **ScÃ©nario : Vente en ligne et magasins physiques**
Une chaÃ®ne de magasins de dÃ©tail opÃ©rant Ã  la fois en ligne et en physique souhaite mieux comprendre ses ventes, ses clients et ses magasins. Pour cela, elle a besoin de :

- **DonnÃ©es transactionnelles (MySQL)** : Commandes clients en ligne (produits achetÃ©s, montants, date).  
- **DonnÃ©es externes (CSV)** : Informations gÃ©ographiques sur les magasins physiques (ville, rÃ©gion, pays).  
- **Centralisation dans un entrepÃ´t de donnÃ©es (PostgreSQL)** pour analyse et reporting.  
- **Transformations analytiques (dbt)** : ModÃ©lisation des ventes par client, produit, et localisation.  
- **Orchestration (Airflow)** : Automatisation des workflows ETL/ELT.  
- **Visualisation (Superset)** : Tableaux de bord interactifs sur les ventes et performances.  
- **Gouvernance (OpenMetadata)** : Documentation, lignage des donnÃ©es, gouvernance.

---

## ğŸ› ï¸ **3. Architecture Technique**

L'architecture de RetailFlow repose sur une approche **modulaire et Ã©volutive** :

## ğŸ“Œ **4. DÃ©tails des Composants :**
1. MySQL (Base de donnÃ©es transactionnelle)
    - Stocke les transactions clients, produits et commandes.

2. Fichier CSV (Localisation des magasins)
    - Fichier externe stockÃ© localement.

3. Airbyte (Ingestion de donnÃ©es)
    - Collecte les donnÃ©es depuis MySQL et le fichier CSV.
    - Envoie les donnÃ©es vers MinIO (Data Lake) et PostgreSQL (Data Warehouse).

4. MinIO (Stockage Objet)
    - Stockage de fichiers bruts et formatÃ©s pour les traitements futurs.
    
5. PostgreSQL (Data Warehouse)
    - Stocke les donnÃ©es modÃ©lisÃ©es pour les requÃªtes analytiques.

6. dbt (Transformation)
    -  Transforme les donnÃ©es en modÃ¨les analytiques (vue agrÃ©gÃ©e des ventes, clients, etc.).

7. Airflow (Orchestration)
    - Automatisation de la collecte (Airbyte), transformation (dbt) et mise Ã  jour des tableaux de bord.

8. OpenMetadata (Gouvernance)
    - Assure la traÃ§abilitÃ© et la documentation des pipelines de donnÃ©es.

9. Superset (Visualisation)
    - CrÃ©e des tableaux de bord interactifs pour explorer les ventes, les clients et les magasins.

## ğŸ“ 4. **Ã‰tapes ClÃ©s du Pipeline de DonnÃ©es**
1. Ingestion avec Airbyte :
    - Synchronisation rÃ©guliÃ¨re des donnÃ©es depuis MySQL et le fichier CSV vers PostgreSQL et MinIO.

2. Stockage :
    - Les donnÃ©es sont centralisÃ©es dans PostgreSQL pour des analyses structurÃ©es.
    - Les fichiers bruts sont stockÃ©s dans MinIO.

3. Transformation avec dbt :
    - AgrÃ©gation des ventes par client, produit et localisation.
    - Nettoyage et structuration des donnÃ©es.

4. Orchestration avec Airflow :
    - Automatisation de la collecte (Airbyte), transformation (dbt) et mise Ã  jour des tableaux de bord.

5. Gouvernance avec OpenMetadata :
    - Documentation automatisÃ©e des schÃ©mas et tables.
    - Lignage des donnÃ©es.

6. Visualisation avec Superset :
    - CrÃ©ation de tableaux de bord pour analyser :
    - Les produits les plus vendus.
    - Les clients les plus rentables.
    - La performance des magasins par rÃ©gion.

## ğŸ“ 5. **Configuration du Projet**
### ğŸ› ï¸ PrÃ©requis :
- Docker & Docker Compose installÃ©s localement.
- Python 3.x pour les scripts de gÃ©nÃ©ration de donnÃ©es.
### ğŸ“¥ **Installation :**
```bash
    git clone https://github.com/username/retail-flow-project.git
    cd retail-flow-project

    # Ã‰tape 1
    cd step1-mysql
    docker-compose up -d
    python data_generator.py

    # Ã‰tape 2
    cd step2-data-engineering
    docker-compose up -d
```

## ğŸ“Š 6. **Tableaux de Bord Attendues**
- Performance des ventes par rÃ©gion.
- Top 10 des clients avec les plus gros achats.
- Produits les plus vendus.

## ğŸ”— 7. **Ressources Utiles**
- Airbyte Documentation: https://docs.airbyte.com
- dbt Documentation: https://docs.getdbt.com
- Apache Superset: https://superset.apache.org


## 8. Arbo
```plaintext
retail-flow-project/
â”‚
â”œâ”€ step1-mysql/
â”‚  â”œâ”€ docker-compose.yml
â”‚  â”œâ”€ init-db.sql
â”‚  â”œâ”€ data_generator.py
â”‚  â””â”€ README.md
â”‚
â”‚â”€step2-data-engineering/
|     â”‚
|     â”œâ”€â”€ docker-compose.yml          # Docker Compose pour Airbyte, MinIO, PostgreSQL, dbt, Airflow, Superset, OpenMetadata
|     â”œâ”€â”€ .env                        # Fichier des variables d'environnement (mots de passe, configurations)
|     â”œâ”€â”€ README.md                   # Documentation de l'Ã©tape 2
|     â”‚
|     â”œâ”€â”€ airbyte/                    # Configuration spÃ©cifique Ã  Airbyte
|     â”‚   â”œâ”€â”€ connections/            # Configuration des connexions Airbyte
|     â”‚   â”‚   â”œâ”€â”€ mysql_to_postgres.json
|     â”‚   â”‚   â”œâ”€â”€ csv_to_minio.json
|     â”‚   â”œâ”€â”€ docker-entrypoint.sh    # Script d'initialisation
|     â”‚   â””â”€â”€ airbyte-config.yml      # Configuration globale d'Airbyte
|     â”‚
|     â”œâ”€â”€ minio/                      # Configuration MinIO
|     â”‚   â”œâ”€â”€ data/                   # Stockage des fichiers bruts CSV (Data Lake)
|     â”‚   â”œâ”€â”€ config.json             # Configuration MinIO
|     â”‚   â””â”€â”€ docker-entrypoint.sh    # Script d'initialisation
|     â”‚
|     â”œâ”€â”€ postgres/                   # Configuration PostgreSQL (Data Warehouse)
|     â”‚   â”œâ”€â”€ init-scripts/           # Scripts d'initialisation PostgreSQL
|     â”‚   â”‚   â”œâ”€â”€ init.sql
|     â”‚   â””â”€â”€ docker-entrypoint.sh    # Script de dÃ©marrage
|     â”‚
|     â”œâ”€â”€ dbt/                        # Projet dbt (Transformation SQL)
|     â”‚   â”œâ”€â”€ dbt_project.yml         # Configuration principale dbt
|     â”‚   â”œâ”€â”€ profiles.yml            # Configuration des connexions dbt
|     â”‚   â”œâ”€â”€ models/                 # ModÃ¨les SQL
|     â”‚   â”‚   â”œâ”€â”€ staging/            # ModÃ¨les de staging
|     â”‚   â”‚   â”‚   â”œâ”€â”€ stg_clients.sql
|     â”‚   â”‚   â”‚   â”œâ”€â”€ stg_produits.sql
|     â”‚   â”‚   â”‚   â”œâ”€â”€ stg_commandes.sql
|     â”‚   â”‚   â”œâ”€â”€ marts/              # ModÃ¨les analytiques
|     â”‚   â”‚   â”‚   â”œâ”€â”€ mart_sales_summary.sql
|     â”‚   â”‚   â”‚   â”œâ”€â”€ mart_top_customers.sql
|     â”‚   â”‚   â”œâ”€â”€ macros/             # Macros SQL personnalisÃ©es
|     â”‚   â”‚   â”œâ”€â”€ tests/              # Tests de qualitÃ© des donnÃ©es
|     â”‚   â”œâ”€â”€ logs/                   # Logs des exÃ©cutions dbt
|     â”‚   â”œâ”€â”€ target/                 # RÃ©sultats compilÃ©s de dbt
|     â”‚   â””â”€â”€ README.md
|     â”‚
|     â”œâ”€â”€ airflow/                    # Configuration Airflow (Orchestration)
|     â”‚   â”œâ”€â”€ dags/                   # DAGs Airflow
|     â”‚   â”‚   â”œâ”€â”€ retail_pipeline_dag.py
|     â”‚   â”‚   â”œâ”€â”€ airbyte_ingestion_dag.py
|     â”‚   â”‚   â”œâ”€â”€ dbt_transformation_dag.py
|     â”‚   â”œâ”€â”€ plugins/                # Plugins personnalisÃ©s pour Airflow
|     â”‚   â”œâ”€â”€ airflow.cfg             # Fichier de configuration d'Airflow
|     â”‚   â”œâ”€â”€ docker-entrypoint.sh    # Script de dÃ©marrage Airflow
|     â”‚   â”œâ”€â”€ logs/                   # Logs d'exÃ©cution des DAGs
|     â”‚   â””â”€â”€ README.md
|     â”‚
|     â”œâ”€â”€ superset/                   # Configuration Superset (Visualisation)
|     â”‚   â”œâ”€â”€ docker-entrypoint.sh    # Script de dÃ©marrage
|     â”‚   â”œâ”€â”€ superset_config.py      # Configuration principale Superset
|     â”‚   â”œâ”€â”€ dashboards/             # Fichiers JSON pour les tableaux de bord
|     â”‚   â”‚   â”œâ”€â”€ sales_dashboard.json
|     â”‚   â”‚   â”œâ”€â”€ product_performance.json
|     â”‚   â”œâ”€â”€ datasets/               # Datasets Superset
|     â”‚   â”œâ”€â”€ logs/                   # Logs Superset
|     â”‚   â””â”€â”€ README.md
|     â”‚
|     â”œâ”€â”€ openmetadata/               # Configuration OpenMetadata (Gouvernance)
|     â”‚   â”œâ”€â”€ docker-entrypoint.sh    # Script de dÃ©marrage
|     â”‚   â”œâ”€â”€ openmetadata_config.yml # Configuration principale
|     â”‚   â”œâ”€â”€ ingestion/              # Pipelines d'ingestion des mÃ©tadonnÃ©es
|     â”‚   â”‚   â”œâ”€â”€ mysql_metadata.yml
|     â”‚   â”‚   â”œâ”€â”€ postgres_metadata.yml
|     â”‚   â”‚   â”œâ”€â”€ superset_metadata.yml
|     â”‚   â”œâ”€â”€ logs/                   # Logs OpenMetadata
|     â”‚   â””â”€â”€ README.md
|     â”‚
|     â”œâ”€â”€ scripts/                    # Scripts utilitaires
|     â”‚   â”œâ”€â”€ reset_pipeline.sh       # Script pour rÃ©initialiser les pipelines
|     â”‚   â”œâ”€â”€ health_check.sh         # Script pour vÃ©rifier l'Ã©tat des services
|     â”‚   â”œâ”€â”€ load_csv_to_minio.sh    # Script pour charger les fichiers CSV dans MinIO
|     â”‚   â””â”€â”€ README.md
|     â”‚
|     â””â”€â”€ logs/                       # Logs gÃ©nÃ©raux
|         â”œâ”€â”€ airbyte.log
|         â”œâ”€â”€ airflow.log
|         â”œâ”€â”€ dbt.log
|         â”œâ”€â”€ superset.log
|         â”œâ”€â”€ openmetadata.log
|         â””â”€â”€ general.log
|   
â”œâ”€ data/
â”‚  â””â”€ store_locations.csv
â”‚
â””â”€ README.md
```